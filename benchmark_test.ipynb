{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark FastText vs PyTorch sur APE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import s3fs\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import pyarrow.parquet as pq\n",
    "import fasttext\n",
    "import os\n",
    "import warnings\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "import unidecode\n",
    "from src.model import FastTextModule, FastTextModel\n",
    "from src.dataset import FastTextModelDataset\n",
    "from src.tokenizer import NGramTokenizer\n",
    "from src.preprocess import clean_text_feature\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_server_uri = mlflow.get_tracking_uri()\n",
    "experiment_name = \"benchmark_fasttext\"\n",
    "run_name=\"\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"y_name\": \"nace\",\n",
    "    \"text_feature\": \"text\",\n",
    "    \"df_sample_size\": 50000,\n",
    "    \"max_epochs\": 50, #50\n",
    "    \"train_proportion\": 0.8,\n",
    "    \"lr\": 0.4,\n",
    "    \"buckets\": 2000000, #2000000\n",
    "    \"dim\": 180, # 180\n",
    "    \"minCount\": 1,\n",
    "    \"minn\": 3,\n",
    "    \"maxn\": 5,\n",
    "    \"wordNgrams\": 3,\n",
    "    \"ft_thread\": 100,\n",
    "    \"ft_loss\": \"ova\", #\"softmax\",\n",
    "    \"ft_lrUpdateRate\": 0, #100\n",
    "    \"ft_neg\": 1000, # 5\n",
    "    \"torch_batch_size\": 64,\n",
    "    \"torch_patience\": 3,\n",
    "    \"torch_sparse\": True,\n",
    "    \"torch_num_workers\": 20\n",
    "}\n",
    "categorical_features_torch=[] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"}, anon=True\n",
    ")\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-formation/diffusion/mlops/data/firm_activity_data.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de valeurs vide : {(df[params[\"y_name\"]]==\"\").sum()}\")\n",
    "print(f\"Nombre de valeurs NA : {df[\"nace\"].isna().sum()}\")\n",
    "\n",
    "df = df.sample(params[\"df_sample_size\"], random_state=123)\n",
    "\n",
    "counts = df[params[\"y_name\"]].value_counts()\n",
    "modalites_suffisantes = counts[counts >= 3].index\n",
    "df = df[df[params[\"y_name\"]].isin(modalites_suffisantes)]\n",
    "\n",
    "print(f\"Shape of sampled df after removal of rare outcomes : {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text feature\n",
    "df = clean_text_feature(df, text_feature=\"text\")\n",
    "\n",
    "# Encode classes\n",
    "encoder = LabelEncoder()\n",
    "df[params[\"y_name\"]] = encoder.fit_transform(df[params[\"y_name\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df[params[\"y_name\"]].value_counts()<3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[params[\"y_name\"]].value_counts()\n",
    "modalites_suffisantes = counts[counts < 3].index\n",
    "print(modalites_suffisantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[[params[\"text_feature\"]]],\n",
    "    df[params[\"y_name\"]],\n",
    "    test_size=1 - params[\"train_proportion\"],\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    "    stratify=df[params[\"y_name\"]]\n",
    ")\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.nunique()\n",
    "print(f\"Nombre de classes dans y_train : {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas 1 : FastText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_training_data(\n",
    "    df: pd.DataFrame,\n",
    "    y: str,\n",
    "    text_feature: str,\n",
    "    categorical_features: Optional[List[str]],\n",
    "    label_prefix: str = \"__label__\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Write training data to file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame.\n",
    "        y (str): Output variable name.\n",
    "        text_feature (str): Text feature.\n",
    "        categorical_features (Optional[List[str]]): Categorical features.\n",
    "        label_prefix (str, optional): Label prefix. Defaults to \"__label__\".\n",
    "\n",
    "    Returns:\n",
    "        str: Training data path.\n",
    "    \"\"\"\n",
    "    training_data_path = Path(\"data/training_data.txt\")\n",
    "\n",
    "    with open(training_data_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for _, item in df.iterrows():\n",
    "            formatted_item = f\"{label_prefix}{item[y]} {item[text_feature]}\"\n",
    "            if categorical_features != []:\n",
    "                for feature in categorical_features:\n",
    "                    formatted_item += f\" {feature}_{item[feature]}\"\n",
    "            file.write(f\"{formatted_item}\\n\")\n",
    "    return training_data_path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training data in a .txt file (fasttext-specific)\n",
    "training_data_path = write_training_data(\n",
    "    df=df_train,\n",
    "    y=params[\"y_name\"],\n",
    "    text_feature=params[\"text_feature\"],\n",
    "    categorical_features=[],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the fasttext model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_ft = fasttext.train_supervised(\n",
    "    input=training_data_path,\n",
    "    dim=params[\"dim\"],\n",
    "    lr=params[\"lr\"],\n",
    "    epoch=params[\"max_epochs\"],\n",
    "    lrUpdateRate=params[\"ft_lrUpdateRate\"],\n",
    "    neg=params[\"ft_neg\"],\n",
    "    wordNgrams=params[\"wordNgrams\"],\n",
    "    minn=params[\"minn\"],\n",
    "    maxn=params[\"maxn\"],\n",
    "    minCount=params[\"minCount\"],\n",
    "    bucket=params[\"buckets\"],\n",
    "    thread=params[\"ft_thread\"],\n",
    "    loss=params[\"ft_loss\"],\n",
    "    label_prefix=\"__label__\",\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_ft = (end_time - start_time) / 60\n",
    "print(\"Temps écoulé pour entrainer la lib fasttext : \", elapsed_time_ft, \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input = []\n",
    "for _, item in df_val.iterrows():\n",
    "    formatted_item = f\"{\"__label__\"}{item[params[\"y_name\"]]} {item[params[\"text_feature\"]]}\"\n",
    "    val_input.append(formatted_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_ft.predict(val_input, k=1)\n",
    "predictions = [x[0].replace(\"__label__\", \"\") for x in predictions[0]]\n",
    "booleans = [\n",
    "    prediction == str(label)\n",
    "    for prediction, label in zip(predictions, df_val[params[\"y_name\"]])\n",
    "]\n",
    "accuracy_ft = sum(booleans) / len(booleans)\n",
    "accuracy_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas 2 : Entraînement et evaluation avec la réimplémentation PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = X_train[params[\"text_feature\"]].to_list()\n",
    "tokenizer = NGramTokenizer(\n",
    "    params['minCount'], params['minn'], params['maxn'], params['buckets'], params['wordNgrams'], training_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_train[column].to_list() for column in X_train[categorical_features_torch]\n",
    "    ],\n",
    "    texts=training_text,\n",
    "    outputs=y_train.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_val[column].to_list() for column in X_val[categorical_features_torch]\n",
    "    ],\n",
    "    texts=X_val[params[\"text_feature\"]].to_list(),\n",
    "    outputs=y_val.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch = FastTextModel(\n",
    "    embedding_dim=params['dim'],\n",
    "    vocab_size=params['buckets'] + tokenizer.get_nwords() + 1,\n",
    "    num_classes=num_classes,\n",
    "    categorical_vocabulary_sizes=[], # use case without add variables\n",
    "    padding_idx=params['buckets'] + tokenizer.get_nwords(),\n",
    "    sparse=params['torch_sparse'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "if params['torch_sparse']:\n",
    "    optimizer = SGD\n",
    "else:\n",
    "    optimizer = Adam\n",
    "optimizer_params = {\"lr\": params['lr']}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {\n",
    "    \"mode\": \"min\",\n",
    "    \"patience\": params['torch_patience'],\n",
    "}\n",
    "\n",
    "\n",
    "# Lightning module\n",
    "module = FastTextModule(\n",
    "    model=model_torch,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=\"epoch\",\n",
    ")\n",
    "\n",
    "# Trainer callbacks\n",
    "checkpoints = [\n",
    "    {\n",
    "        \"monitor\": \"validation_loss\",\n",
    "        \"save_top_k\": 1,\n",
    "        \"save_last\": False,\n",
    "        \"mode\": \"min\",\n",
    "    }\n",
    "]\n",
    "callbacks = [ModelCheckpoint(**checkpoint) for checkpoint in checkpoints]\n",
    "callbacks.append(\n",
    "    EarlyStopping(\n",
    "        monitor=\"validation_loss\",\n",
    "        patience=params['torch_patience'],\n",
    "        mode=\"min\",\n",
    "    )\n",
    ")\n",
    "callbacks.append(LearningRateMonitor(logging_interval=\"step\"))\n",
    "\n",
    "# Strategy\n",
    "strategy = \"auto\"\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=params['max_epochs'],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    ")\n",
    "\n",
    "# Training\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.get_num_threads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"threads avant fit = {torch.get_num_threads()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.fit(module, train_dataloader, val_dataloader)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_torch = (end_time - start_time) / 60\n",
    "print(f\"threads après fit = {torch.get_num_threads()}\")\n",
    "print(\"Temps écoulé pour entrainer la réimplementation PyTorch : \", elapsed_time_torch, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer le modèle en mode évaluation\n",
    "model_torch.eval()\n",
    "\n",
    "# Initialiser les listes pour stocker les vraies valeurs et les prédictions\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# Boucle d'évaluation sur le DataLoader de test\n",
    "with torch.no_grad():  # Pas de calcul de gradient lors de l'évaluation\n",
    "    for batch in val_dataloader:\n",
    "        inputs, labels = batch[:-1], batch[-1]\n",
    "        # Obtenir les prédictions\n",
    "        outputs = model_torch(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtenir les classes prédictes\n",
    "        \n",
    "        # Ajouter les labels et les prédictions aux listes\n",
    "        all_labels.extend(labels.numpy())  # Pas besoin de .cpu() car tu es sur CPU\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "# Calcul des métriques avec scikit-learn\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')  # 'weighted' pour la moyenne pondérée par classe\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy de la lib Fasttext: {accuracy_ft}\")\n",
    "print(f\"Accuracy de la réimplémentation PyTorch: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification sur la structure de chaque modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_expected = params[\"dim\"] * (params[\"buckets\"] + tokenizer.get_nwords() + 1) + ((params[\"dim\"] * num_classes) + num_classes)\n",
    "torch_total_params = sum(p.numel() for p in model_torch.parameters())\n",
    "ft_embedding_dim = model_ft.get_input_matrix().shape[1]\n",
    "\n",
    "ft_nb_labels = len(model_ft.get_labels())\n",
    "ft_nb_words = len(model_ft.get_words())\n",
    "\n",
    "ft_vocab_size = model_ft.get_input_matrix().shape[0]\n",
    "torch_vocab_size = model_torch.embeddings.weight.shape[0]\n",
    "\n",
    "ft_total_params = ft_vocab_size * ft_embedding_dim + (ft_embedding_dim * ft_nb_labels + ft_nb_labels)\n",
    "\n",
    "print(f\"Nombre de labels d'après FastText = {ft_nb_labels} ({num_classes} attendus)\")\n",
    "print(f\"Nombre de mots d'après FastText = {ft_nb_words} et d'après Torch = {tokenizer.get_nwords()}\")\n",
    "\n",
    "print(f\"Nombre de tokens d'après FastText = {ft_vocab_size}\") \n",
    "print(f\"Nombre de tokens d'après Torch = {torch_vocab_size}\") \n",
    "\n",
    "print(f\"Nombre total de paramètres dans Torch : {torch_total_params}\") \n",
    "print(f\"Nombre total de paramètres dans Fasttext (attendu) : {ft_total_params}\") \n",
    "print(f\"Nombre de paramètres attendus en théorie : {total_params_expected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging sur MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.log_metric(\"accuracy_torch\", accuracy)\n",
    "    mlflow.log_metric(\"accuracy_fasttext\", accuracy_ft)\n",
    "    mlflow.log_metric(\"time_fasttext\", elapsed_time_ft)\n",
    "    mlflow.log_metric(\"time_torch\", elapsed_time_torch)\n",
    "    for param_name in sorted(params.keys()):\n",
    "         mlflow.log_param(param_name, params[param_name])\n",
    "    mlflow.log_param(\"categorical_features_torch\", categorical_features_torch)\n",
    "    mlflow.log_artifact(\"requirements.txt\")\n",
    "    mlflow.log_artifacts(\"src/\", artifact_path=\"src\")\n",
    "    mlflow.log_artifact(\"./benchmark_test.ipynb\", artifact_path=\"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sur la réimplémentation torch :\n",
    "    - Pourquoi pas de référence à softmax, ova, etc. dans la définition du modèles torch ?\n",
    "- Sur le modèle de la lib FastText : \n",
    "    - comment on fait du negative sampling en même temps que le classifier ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
