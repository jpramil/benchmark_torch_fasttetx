{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark des ajouts de variables catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import s3fs\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import pyarrow.parquet as pq\n",
    "import fasttext\n",
    "import os\n",
    "import warnings\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "import unidecode\n",
    "# from src.model_negsamp import FastTextModule_negsamp, FastTextModel_negsamp\n",
    "from src.model import FastTextModule, FastTextModel\n",
    "from src.dataset import FastTextModelDataset\n",
    "from src.tokenizer import NGramTokenizer\n",
    "from src.preprocess import clean_text_feature\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_server_uri = mlflow.get_tracking_uri()\n",
    "experiment_name = \"benchmark_categorial\"\n",
    "run_name=\"\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging local du modèle avec TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard loggers\n",
    "path_logger_torch = \"model_torch\"\n",
    "path_logger_torch_tweeked = \"model_torch_tweeked\"\n",
    "tb_logger = TensorBoardLogger(\"logs\", name=path_logger_torch)\n",
    "tb_logger_libelle_tweeked = TensorBoardLogger(\"logs\", name=path_logger_torch_tweeked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"y_name\": \"nace2025\",\n",
    "    \"text_feature\": \"libelle\",\n",
    "    \"text_feature_tweeked\": \"libelle_tweeked\",\n",
    "    \"df_sample_size\": 50000,\n",
    "    \"max_epochs\": 5, #50\n",
    "    \"train_proportion\": 0.8,\n",
    "    \"lr\": 0.004,\n",
    "    \"buckets\": 2000000, #2000000\n",
    "    \"dim\": 200, # 180\n",
    "    \"minCount\": 1,\n",
    "    \"minn\": 3,\n",
    "    \"maxn\": 5,\n",
    "    \"wordNgrams\": 3,\n",
    "    \"ft_thread\": 100,\n",
    "    \"ft_loss\": \"softmax\", #\"softmax\",\"ova\"\n",
    "    \"ft_lrUpdateRate\": 100, #100\n",
    "    \"ft_neg\": 5, # 5\n",
    "    \"torch_batch_size\": 64,\n",
    "    \"torch_patience\": 3,\n",
    "    \"torch_sparse\": True,\n",
    "    \"torch_num_workers\": 4,\n",
    "    \"categorical_features\": [\"activ_nat_et\", \"liasse_type\"] ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-ape/NAF-revision/relabeled-data/20241027_sirene4_nace2025.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de valeurs vide : {(df[params[\"y_name\"]]==\"\").sum()}\")\n",
    "print(f\"Nombre de valeurs NA : {df[params[\"y_name\"]].isna().sum()}\")\n",
    "\n",
    "df = df.dropna(subset=[params[\"y_name\"]])\n",
    "\n",
    "df = df.sample(params[\"df_sample_size\"], random_state=123)\n",
    "\n",
    "counts = df[params[\"y_name\"]].value_counts()\n",
    "modalites_suffisantes = counts[counts >= 3].index\n",
    "df = df[df[params[\"y_name\"]].isin(modalites_suffisantes)]\n",
    "\n",
    "print(f\"Shape of sampled df after removal of rare outcomes : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text feature\n",
    "df = clean_text_feature(df, text_feature=params[\"text_feature\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout d'une variable textuelle de concaténation du libellé textuel et des variables catégorielles (astuce utilisée avec la lib fasttext dans les modèles en prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[params['text_feature_tweeked']]=''\n",
    "for idx, item in df.iterrows():\n",
    "    formatted_item = item[params['text_feature']]\n",
    "    if params[\"categorical_features\"] != []:\n",
    "        for feature in params[\"categorical_features\"]:\n",
    "            formatted_item += f\" {feature}_{item[feature]}\"\n",
    "    print(formatted_item)\n",
    "    df.at[idx, params['text_feature_tweeked']] = formatted_item\n",
    "\n",
    "df[params['text_feature_tweeked']].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode outputs and categorical variables\n",
    "encoder = LabelEncoder()\n",
    "df[params[\"y_name\"]] = encoder.fit_transform(df[params[\"y_name\"]])\n",
    "\n",
    "for var_categ_name in params[\"categorical_features\"]:\n",
    "    encoder = LabelEncoder()\n",
    "    df[var_categ_name] = encoder.fit_transform(df[var_categ_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "print((df[params[\"y_name\"]].value_counts()<3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[[params[\"text_feature\"], params[\"text_feature_tweeked\"]] + params[\"categorical_features\"]],\n",
    "    df[params[\"y_name\"]],\n",
    "    test_size=1 - params[\"train_proportion\"],\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    "    stratify=df[params[\"y_name\"]]\n",
    ")\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.nunique()\n",
    "print(f\"Nombre de classes dans y_train : {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas 1 : FastText "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tweek le libelle textuel en entrée de Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_training_data(\n",
    "    df: pd.DataFrame,\n",
    "    y: str,\n",
    "    text_feature: str,\n",
    "    categorical_features: Optional[List[str]],\n",
    "    label_prefix: str = \"__label__\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Write training data to file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame.\n",
    "        y (str): Output variable name.\n",
    "        text_feature (str): Text feature.\n",
    "        categorical_features (Optional[List[str]]): Categorical features.\n",
    "        label_prefix (str, optional): Label prefix. Defaults to \"__label__\".\n",
    "\n",
    "    Returns:\n",
    "        str: Training data path.\n",
    "    \"\"\"\n",
    "    training_data_path = Path(\"data/training_data.txt\")\n",
    "\n",
    "    with open(training_data_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for _, item in df.iterrows():\n",
    "            formatted_item = f\"{label_prefix}{item[y]} {item[text_feature]}\"\n",
    "            if categorical_features != []:\n",
    "                for feature in categorical_features:\n",
    "                    formatted_item += f\" {feature}_{item[feature]}\"\n",
    "            file.write(f\"{formatted_item}\\n\")\n",
    "    return training_data_path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training data in a .txt file (fasttext-specific)\n",
    "training_data_path = write_training_data(\n",
    "    df=df_train,\n",
    "    y=params[\"y_name\"],\n",
    "    text_feature=params[\"text_feature\"],\n",
    "    categorical_features=params[\"categorical_features\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the fasttext model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_ft = fasttext.train_supervised(\n",
    "    input=training_data_path,\n",
    "    dim=params[\"dim\"],\n",
    "    lr=params[\"lr\"],\n",
    "    epoch=params[\"max_epochs\"],\n",
    "    lrUpdateRate=params[\"ft_lrUpdateRate\"],\n",
    "    neg=params[\"ft_neg\"],\n",
    "    wordNgrams=params[\"wordNgrams\"],\n",
    "    minn=params[\"minn\"],\n",
    "    maxn=params[\"maxn\"],\n",
    "    minCount=params[\"minCount\"],\n",
    "    bucket=params[\"buckets\"],\n",
    "    thread=params[\"ft_thread\"],\n",
    "    loss=params[\"ft_loss\"],\n",
    "    label_prefix=\"__label__\",\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_ft = (end_time - start_time) / 60\n",
    "print(\"Temps écoulé pour entrainer la lib fasttext : \", elapsed_time_ft, \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input = []\n",
    "for _, item in df_val.iterrows():\n",
    "    formatted_item = f\"{\"__label__\"}{item[params[\"y_name\"]]} {item[params[\"text_feature\"]]}\"\n",
    "    val_input.append(formatted_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_ft.predict(val_input, k=1)\n",
    "predictions = [x[0].replace(\"__label__\", \"\") for x in predictions[0]]\n",
    "booleans = [\n",
    "    prediction == str(label)\n",
    "    for prediction, label in zip(predictions, df_val[params[\"y_name\"]])\n",
    "]\n",
    "accuracy_ft = sum(booleans) / len(booleans)\n",
    "accuracy_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas 2 : Réimplémentation PyTorch avec intégration des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texte d'origine et variable additionnelles proprement intégrées au modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = X_train[params[\"text_feature\"]].to_list()\n",
    "tokenizer = NGramTokenizer(\n",
    "    params['minCount'], params['minn'], params['maxn'], params['buckets'], params['wordNgrams'], training_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_train[column].to_list() for column in X_train[params[\"categorical_features\"]]\n",
    "    ],\n",
    "    texts=training_text,\n",
    "    outputs=y_train.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_val[column].to_list() for column in X_val[params[\"categorical_features\"]]\n",
    "    ],\n",
    "    texts=X_val[params[\"text_feature\"]].to_list(),\n",
    "    outputs=y_val.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df[params[\"y_name\"]].nunique()\n",
    "categorical_vocabulary_sizes = [\n",
    "    len(np.unique(X_train[feature])) for feature in params[\"categorical_features\"]\n",
    "]\n",
    "print(categorical_vocabulary_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch = FastTextModel(\n",
    "    embedding_dim=params['dim'],\n",
    "    vocab_size=params['buckets'] + tokenizer.get_nwords() + 1,\n",
    "    num_classes=num_classes,\n",
    "    categorical_vocabulary_sizes=categorical_vocabulary_sizes,\n",
    "    padding_idx=params['buckets'] + tokenizer.get_nwords(),\n",
    "    sparse=params['torch_sparse'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "if params['torch_sparse']:\n",
    "    optimizer = SGD\n",
    "else:\n",
    "    optimizer = Adam\n",
    "optimizer_params = {\"lr\": params['lr']}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {\n",
    "    \"mode\": \"min\",\n",
    "    \"patience\": params['torch_patience'],\n",
    "}\n",
    "\n",
    "\n",
    "# Lightning module\n",
    "module_torch = FastTextModule(\n",
    "    model=model_torch,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=\"epoch\",\n",
    ")\n",
    "\n",
    "# Trainer callbacks\n",
    "checkpoints = [\n",
    "    {\n",
    "        \"monitor\": \"validation_loss\",\n",
    "        \"save_top_k\": 1,\n",
    "        \"save_last\": False,\n",
    "        \"mode\": \"min\",\n",
    "    }\n",
    "]\n",
    "callbacks = [ModelCheckpoint(**checkpoint) for checkpoint in checkpoints]\n",
    "callbacks.append(\n",
    "    EarlyStopping(\n",
    "        monitor=\"validation_loss\",\n",
    "        patience=params['torch_patience'],\n",
    "        mode=\"min\",\n",
    "    )\n",
    ")\n",
    "callbacks.append(LearningRateMonitor(logging_interval=\"step\"))\n",
    "\n",
    "# Strategy\n",
    "strategy = \"auto\"\n",
    "\n",
    "# Trainer\n",
    "trainer_torch = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=params['max_epochs'],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    "    logger=tb_logger,\n",
    ")\n",
    "\n",
    "# Training\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.get_num_threads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"threads avant fit = {torch.get_num_threads()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer_torch.fit(module_torch, train_dataloader, val_dataloader)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_torch = (end_time - start_time) / 60\n",
    "print(f\"threads après fit = {torch.get_num_threads()}\")\n",
    "print(\"Temps écoulé pour entrainer la réimplementation PyTorch : \", elapsed_time_torch, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer le modèle en mode évaluation\n",
    "model_torch.eval()\n",
    "\n",
    "# Initialiser les listes pour stocker les vraies valeurs et les prédictions\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# Boucle d'évaluation sur le DataLoader de test\n",
    "with torch.no_grad():  # Pas de calcul de gradient lors de l'évaluation\n",
    "    for batch in val_dataloader:\n",
    "        inputs, labels = batch[:-1], batch[-1]\n",
    "        # Obtenir les prédictions\n",
    "        outputs = model_torch(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtenir les classes prédictes\n",
    "        \n",
    "        # Ajouter les labels et les prédictions aux listes\n",
    "        all_labels.extend(labels.numpy())  # Pas besoin de .cpu() car tu es sur CPU\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "# Calcul des métriques avec scikit-learn\n",
    "accuracy_torch = accuracy_score(all_labels, all_preds)\n",
    "precision_torch = precision_score(all_labels, all_preds, average='weighted')  # 'weighted' pour la moyenne pondérée par classe\n",
    "recall_torch = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1_torch = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_torch:.4f}\")\n",
    "print(f\"Precision: {precision_torch:.4f}\")\n",
    "print(f\"Recall: {recall_torch:.4f}\")\n",
    "print(f\"F1 Score: {f1_torch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas 3 : Torch avec intégration des variables catégorielles dans le libellé textuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_tweeked = X_train[params[\"text_feature_tweeked\"]].to_list()\n",
    "tokenizer = NGramTokenizer(\n",
    "    params['minCount'], params['minn'], params['maxn'], params['buckets'], params['wordNgrams'], training_text_tweeked\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_train[column].to_list() for column in X_train[[]] # empty list\n",
    "    ],\n",
    "    texts=training_text_tweeked,\n",
    "    outputs=y_train.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_val[column].to_list() for column in X_val[[]]\n",
    "    ],\n",
    "    texts=X_val[params[\"text_feature_tweeked\"]].to_list(),\n",
    "    outputs=y_val.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df[params[\"y_name\"]].nunique()\n",
    "categorical_vocabulary_sizes = [\n",
    "    len(np.unique(X_train[feature])) for feature in [] #empty list\n",
    "]\n",
    "print(categorical_vocabulary_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch_libelle_tweeked = FastTextModel(\n",
    "    embedding_dim=params['dim'],\n",
    "    vocab_size=params['buckets'] + tokenizer.get_nwords() + 1,\n",
    "    num_classes=num_classes,\n",
    "    categorical_vocabulary_sizes=categorical_vocabulary_sizes,\n",
    "    padding_idx=params['buckets'] + tokenizer.get_nwords(),\n",
    "    sparse=params['torch_sparse'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "if params['torch_sparse']:\n",
    "    optimizer = SGD\n",
    "else:\n",
    "    optimizer = Adam\n",
    "optimizer_params = {\"lr\": params['lr']}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {\n",
    "    \"mode\": \"min\",\n",
    "    \"patience\": params['torch_patience'],\n",
    "}\n",
    "\n",
    "\n",
    "# Lightning module\n",
    "module_libelle_tweeked = FastTextModule(\n",
    "    model=model_torch_libelle_tweeked,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=\"epoch\",\n",
    ")\n",
    "\n",
    "# Trainer callbacks\n",
    "checkpoints = [\n",
    "    {\n",
    "        \"monitor\": \"validation_loss\",\n",
    "        \"save_top_k\": 1,\n",
    "        \"save_last\": False,\n",
    "        \"mode\": \"min\",\n",
    "    }\n",
    "]\n",
    "callbacks = [ModelCheckpoint(**checkpoint) for checkpoint in checkpoints]\n",
    "callbacks.append(\n",
    "    EarlyStopping(\n",
    "        monitor=\"validation_loss\",\n",
    "        patience=params['torch_patience'],\n",
    "        mode=\"min\",\n",
    "    )\n",
    ")\n",
    "callbacks.append(LearningRateMonitor(logging_interval=\"step\"))\n",
    "\n",
    "# Strategy\n",
    "strategy = \"auto\"\n",
    "\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=params['max_epochs'],\n",
    "    num_sanity_val_steps=2,\n",
    "    strategy=strategy,\n",
    "    log_every_n_steps=2,\n",
    "    logger=tb_logger_libelle_tweeked,\n",
    ")\n",
    "\n",
    "# Training\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.get_num_threads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"threads avant fit = {torch.get_num_threads()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.fit(module_libelle_tweeked, train_dataloader, val_dataloader)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_torch_tweeked = (end_time - start_time) / 60\n",
    "print(f\"threads après fit = {torch.get_num_threads()}\")\n",
    "print(\"Temps écoulé pour entrainer la réimplementation PyTorch tweeked : \", elapsed_time_torch_tweeked, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer le modèle en mode évaluation\n",
    "model_torch_libelle_tweeked.eval()\n",
    "\n",
    "# Initialiser les listes pour stocker les vraies valeurs et les prédictions\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# Boucle d'évaluation sur le DataLoader de test\n",
    "with torch.no_grad():  # Pas de calcul de gradient lors de l'évaluation\n",
    "    for batch in val_dataloader:\n",
    "        inputs, labels = batch[:-1], batch[-1]\n",
    "        # Obtenir les prédictions\n",
    "        outputs = model_torch_libelle_tweeked(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtenir les classes prédictes\n",
    "        \n",
    "        # Ajouter les labels et les prédictions aux listes\n",
    "        all_labels.extend(labels.numpy())  # Pas besoin de .cpu() car tu es sur CPU\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "# Calcul des métriques avec scikit-learn\n",
    "accuracy_libelle_tweeked = accuracy_score(all_labels, all_preds)\n",
    "precision_libelle_tweeked = precision_score(all_labels, all_preds, average='weighted')  # 'weighted' pour la moyenne pondérée par classe\n",
    "recall_libelle_tweeked = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1_libelle_tweeked = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_libelle_tweeked:.4f}\")\n",
    "print(f\"Precision: {precision_libelle_tweeked:.4f}\")\n",
    "print(f\"Recall: {recall_libelle_tweeked:.4f}\")\n",
    "print(f\"F1 Score: {f1_libelle_tweeked:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Accuracy de la lib Fasttext: {accuracy_ft}\")\n",
    "# print(f\"Accuracy de la réimplémentation PyTorch: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification sur la structure de chaque modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params_expected = params[\"dim\"] * (params[\"buckets\"] + tokenizer.get_nwords() + 1) + ((params[\"dim\"] * num_classes) + num_classes)\n",
    "# torch_total_params = sum(p.numel() for p in model_torch.parameters())\n",
    "# ft_embedding_dim = model_ft.get_input_matrix().shape[1]\n",
    "\n",
    "# ft_nb_labels = len(model_ft.get_labels())\n",
    "# ft_nb_words = len(model_ft.get_words())\n",
    "\n",
    "# ft_vocab_size = model_ft.get_input_matrix().shape[0]\n",
    "# torch_vocab_size = model_torch.embeddings.weight.shape[0]\n",
    "\n",
    "# ft_total_params = ft_vocab_size * ft_embedding_dim + (ft_embedding_dim * ft_nb_labels + ft_nb_labels)\n",
    "\n",
    "# print(f\"Nombre de labels d'après FastText = {ft_nb_labels} ({num_classes} attendus)\")\n",
    "# print(f\"Nombre de mots d'après FastText = {ft_nb_words} et d'après Torch = {tokenizer.get_nwords()}\")\n",
    "\n",
    "# print(f\"Nombre de tokens d'après FastText = {ft_vocab_size}\") \n",
    "# print(f\"Nombre de tokens d'après Torch = {torch_vocab_size}\") \n",
    "\n",
    "# print(f\"Nombre total de paramètres dans Torch : {torch_total_params}\") \n",
    "# print(f\"Nombre total de paramètres dans Fasttext (attendu) : {ft_total_params}\") \n",
    "# print(f\"Nombre de paramètres attendus en théorie : {total_params_expected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphiques des accuracy d'entrainement et de validation au cours des epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_chart(log_dir):\n",
    "    # Chercher tous les fichiers `.tfevents` dans le dossier de logs\n",
    "    tfevents_files = glob.glob(os.path.join(log_dir, \"**\", \"*.tfevents.*\"), recursive=True)\n",
    "    \n",
    "    if not tfevents_files:\n",
    "        raise ValueError(\"No tfevents file found\")\n",
    "\n",
    "    # Trouver le fichier le plus récent en triant par date de modification\n",
    "    most_recent_file = max(tfevents_files, key=os.path.getmtime)\n",
    "    print(f\"Most recent tfevent found: {most_recent_file}\")\n",
    "\n",
    "    # Charger les événements du fichier\n",
    "    ea = event_accumulator.EventAccumulator(most_recent_file)\n",
    "    ea.Reload()  # Charge les événements\n",
    "\n",
    "    # Récupérer les tags disponibles\n",
    "    tags = ea.Tags()['scalars']\n",
    "    print(f\"Les tags disponibles : {tags}\")\n",
    "    # Extraire les données pour train_accuracy et val_accuracy\n",
    "    train_accuracy_data = ea.Scalars('train_accuracy_epoch') if 'train_accuracy_epoch' in tags else []\n",
    "    val_accuracy_data = ea.Scalars('validation_accuracy') if 'validation_accuracy' in tags else []\n",
    "\n",
    "    # Extraire les étapes et les valeurs pour chaque métrique si elles existent\n",
    "    if train_accuracy_data:\n",
    "        train_accuracy_batches = [e.step for e in train_accuracy_data]\n",
    "        train_accuracies = [e.value for e in train_accuracy_data]\n",
    "    else:\n",
    "        raise ValueError(\"train_accuracy_epoch not found in tfevent tags\")\n",
    "\n",
    "    if val_accuracy_data:\n",
    "        val_accuracy_batches = [e.step for e in val_accuracy_data]\n",
    "        val_accuracies = [e.value for e in val_accuracy_data]\n",
    "    else:\n",
    "        raise ValueError(\"validation_accuracy not found in tfevent tags\")\n",
    "\n",
    "    # Création de la figure\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Tracer la courbe de l'accuracy d'entraînement\n",
    "    if train_accuracy_batches and train_accuracies:\n",
    "        plt.plot(train_accuracy_batches, train_accuracies, label='Train Accuracy', color='blue')\n",
    "\n",
    "    # Tracer la courbe de l'accuracy de validation\n",
    "    if val_accuracy_batches and val_accuracies:\n",
    "        plt.plot(val_accuracy_batches, val_accuracies, label='Validation Accuracy', color='red')\n",
    "\n",
    "    # Configuration des axes et du titre\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Evolution de l\\'accuracy au cours des epochs (Train vs Validation)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/\" + path_logger_torch\n",
    "fig_torch = get_accuracy_chart(log_dir)\n",
    "fig_torch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_tweek = \"logs/\" + path_logger_torch_tweeked\n",
    "fig_torch_tweek = get_accuracy_chart(log_dir_tweek)\n",
    "fig_torch_tweek.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging sur MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.log_metric(\"accuracy_libelle_tweeked\", accuracy_libelle_tweeked)\n",
    "    mlflow.log_metric(\"precision_libelle_tweeked\", precision_libelle_tweeked)\n",
    "    mlflow.log_metric(\"recall_libelle_tweeked\", recall_libelle_tweeked)\n",
    "    mlflow.log_metric(\"f1_libelle_tweeked\", f1_libelle_tweeked)\n",
    "    mlflow.log_metric(\"accuracy_libelle_tweeked\", accuracy_libelle_tweeked)\n",
    "    mlflow.log_metric(\"precision_libelle_tweeked\", precision_libelle_tweeked)\n",
    "    mlflow.log_metric(\"recall_libelle_tweeked\", recall_libelle_tweeked)\n",
    "    mlflow.log_metric(\"f1_libelle_tweeked\", f1_libelle_tweeked)\n",
    "    mlflow.log_metric(\"accuracy_fasttext\", accuracy_ft)\n",
    "    mlflow.log_metric(\"time_fasttext\", elapsed_time_ft)\n",
    "    mlflow.log_metric(\"time_torch\", elapsed_time_torch)\n",
    "    mlflow.log_metric(\"time_torch_tweeked\", elapsed_time_torch_tweeked)\n",
    "    for param_name in sorted(params):\n",
    "        mlflow.log_param(param_name, params[param_name])\n",
    "    mlflow.log_artifact(\"requirements.txt\")\n",
    "    mlflow.log_artifacts(\"src/\", artifact_path=\"src\")\n",
    "    mlflow.log_artifact(\"./benchmark_test.ipynb\", artifact_path=\"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sur la réimplémentation torch :\n",
    "    - Pourquoi pas de référence à softmax, ova, etc. dans la définition du modèles torch ?\n",
    "- Sur le modèle de la lib FastText : \n",
    "    - comment on fait du negative sampling en même temps que le classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo : \n",
    "- loss curves (adaptation du module lightning pour conserver les loss des epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
