{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import s3fs\n",
    "from typing import List, Optional, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import pyarrow.parquet as pq\n",
    "import fasttext\n",
    "import os\n",
    "import warnings\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "import unidecode\n",
    "# from src.model_negsamp import FastTextModule_negsamp, FastTextModel_negsamp\n",
    "from src.model import FastTextModule, FastTextModel\n",
    "from src.dataset import FastTextModelDataset\n",
    "from src.tokenizer import NGramTokenizer\n",
    "from src.preprocess import clean_text_feature\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device available: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"y_name\": \"nace2025\",\n",
    "    \"text_feature\": \"libelle\",\n",
    "    \"text_feature_tweeked\": \"libelle_tweeked\",\n",
    "    \"df_sample_size\": 100000,\n",
    "    \"max_epochs\": 50, #50\n",
    "    \"train_proportion\": 0.8,\n",
    "    \"buckets\": 2000000, #2000000\n",
    "    \"dim\": 180, # 180\n",
    "    \"minCount\": 1,\n",
    "    \"minn\": 3,\n",
    "    \"maxn\": 6,\n",
    "    \"wordNgrams\": 3,\n",
    "    \"ft_lr\": 0.4,\n",
    "    \"ft_thread\": 100,\n",
    "    \"ft_loss\": \"softmax\", #\"softmax\",\"ova\"\n",
    "    \"ft_lrUpdateRate\": 100, #100\n",
    "    \"ft_neg\": 5, # 5\n",
    "    \"torch_lr\": 0.4,\n",
    "    \"torch_batch_size\": 256,\n",
    "    \"torch_patience_scheduler\": 1,\n",
    "    \"torch_patience_EarlyStopping\": 5,\n",
    "    \"torch_sparse\": False,\n",
    "    \"torch_num_workers\": 100,\n",
    "    # \"categorical_features\": [] ,\n",
    "    \"categorical_features\": [\"activ_nat_et\", \"liasse_type\"] ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-ape/NAF-revision/relabeled-data/20241027_sirene4_nace2025.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "print(f\"Nombre de valeurs vide : {(df[params[\"y_name\"]]==\"\").sum()}\")\n",
    "print(f\"Nombre de valeurs NA : {df[params[\"y_name\"]].isna().sum()}\")\n",
    "\n",
    "df = df.dropna(subset=[params[\"y_name\"]])\n",
    "\n",
    "df = df.sample(params[\"df_sample_size\"], random_state=123)\n",
    "\n",
    "counts = df[params[\"y_name\"]].value_counts()\n",
    "modalites_suffisantes = counts[counts >= 3].index\n",
    "df = df[df[params[\"y_name\"]].isin(modalites_suffisantes)]\n",
    "\n",
    "print(f\"Shape of sampled df after removal of rare outcomes : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text feature\n",
    "df = clean_text_feature(df, text_feature=params[\"text_feature\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout d'une variable textuelle de concaténation du libellé textuel et des variables catégorielles (astuce utilisée avec la lib fasttext dans les modèles en prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[params['text_feature_tweeked']]=''\n",
    "for idx, item in df.iterrows():\n",
    "    formatted_item = item[params['text_feature']]\n",
    "    if params[\"categorical_features\"] != []:\n",
    "        for feature in params[\"categorical_features\"]:\n",
    "            formatted_item += f\" {feature}_{item[feature]}\"\n",
    "    df.at[idx, params['text_feature_tweeked']] = formatted_item\n",
    "\n",
    "df[params['text_feature_tweeked']].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode outputs and categorical variables\n",
    "encoder = LabelEncoder()\n",
    "df[params[\"y_name\"]] = encoder.fit_transform(df[params[\"y_name\"]])\n",
    "\n",
    "for var_categ_name in params[\"categorical_features\"]:\n",
    "    encoder = LabelEncoder()\n",
    "    df[var_categ_name] = encoder.fit_transform(df[var_categ_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[[params[\"text_feature\"], params[\"text_feature_tweeked\"]] + params[\"categorical_features\"]],\n",
    "    df[params[\"y_name\"]],\n",
    "    test_size=1 - params[\"train_proportion\"],\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    "    stratify=df[params[\"y_name\"]]\n",
    ")\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.nunique()\n",
    "print(f\"Nombre de classes dans y_train : {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des inputs du modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texte d'origine et variable additionnelles proprement intégrées au modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = X_train[params[\"text_feature\"]].to_list()\n",
    "tokenizer = NGramTokenizer(\n",
    "    params['minCount'], params['minn'], params['maxn'], params['buckets'], params['wordNgrams'], training_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_train[column].to_list() for column in X_train[params[\"categorical_features\"]]\n",
    "    ],\n",
    "    texts=training_text,\n",
    "    outputs=y_train.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_val[column].to_list() for column in X_val[params[\"categorical_features\"]]\n",
    "    ],\n",
    "    texts=X_val[params[\"text_feature\"]].to_list(),\n",
    "    outputs=y_val.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=params['torch_batch_size'], num_workers=params[\"torch_num_workers\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df[params[\"y_name\"]].nunique()\n",
    "categorical_vocabulary_sizes = [\n",
    "    len(np.unique(X_train[feature])) for feature in params[\"categorical_features\"]\n",
    "]\n",
    "print(categorical_vocabulary_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim=params['dim']\n",
    "vocab_size=params['buckets'] + tokenizer.get_nwords() + 1\n",
    "num_classes=num_classes\n",
    "categorical_vocabulary_sizes=categorical_vocabulary_sizes\n",
    "padding_idx=params['buckets'] + tokenizer.get_nwords()\n",
    "sparse=params['torch_sparse']\n",
    "\n",
    "\n",
    "print(\"Model Initialization Parameters:\")\n",
    "print(f\"embedding_dim: {embedding_dim}\")\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "print(f\"num_classes: {num_classes}\")\n",
    "print(f\"categorical_vocabulary_sizes: {categorical_vocabulary_sizes}\")\n",
    "print(f\"padding_idx: {padding_idx}\")\n",
    "print(f\"sparse: {sparse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_num_classes = num_classes\n",
    "self_padding_idx = padding_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self_embeddings = nn.Embedding(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_embeddings=vocab_size,\n",
    "    padding_idx=padding_idx,\n",
    "    sparse=sparse,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self_categorical_embeddings = {}\n",
    "for var_idx, vocab_size in enumerate(categorical_vocabulary_sizes):\n",
    "    # emb = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=vocab_size)\n",
    "    variable_name = f\"emb_{var_idx}\"\n",
    "    self_categorical_embeddings[variable_name] = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=vocab_size)\n",
    "\n",
    "self_fc = nn.Linear(embedding_dim, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les inputs : dans chaque batch, chaque résumé textuel est transformé en un vecteur de taille : \"nombre de tokens maximum des documents du batch\".\n",
    "Le veceur d'un document contient les index des tokens du documents dans la matrice d'embedding.\n",
    "Ainsi, les index vont de 0 à num_embeddings.\n",
    "Généralement, le vecteur se terminer par plusieurs tokens de padding (le faux token qui sert à faire en sorte que, dans un batch, tous les vecteurs représentant un résumé textuel aient la même taille)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = inputs[0]\n",
    "print(x_1)\n",
    "print(f\"shape: {x_1.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection, pour chaque résumé textuel, du vecteur d'embedding de chacun de ses tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = self_embeddings(x_1)\n",
    "print(x_1)\n",
    "print(f\"shape: {x_1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_categorical_embeddings.items()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque variable additionnelle catégorielle, on extrait le vecteur d'embedding de la valeure prise.\n",
    "Remarque : \"i+1\" car le premier élément de inputs est le tensor des résumés textuels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = []\n",
    "for i, (variable, embedding_layer) in enumerate(\n",
    "    self_categorical_embeddings.items()\n",
    "):\n",
    "    x_cat.append(embedding_layer(inputs[i + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_cat))\n",
    "print(x_cat[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_1.sum(-1))\n",
    "print((x_1.sum(-1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à connaître, pour chaque document textuel d'un batch, le nombre de vrais tokens composant le document textuel.\n",
    "Ainsi, on cherche à connaître le nombre de tokens non-padding pour chaque document textuel.\n",
    "Les tokens padding ont pour vecteur d'embedding un vecteur de zéros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_tokens = x_1.sum(-1) != 0\n",
    "print(non_zero_tokens)\n",
    "print(non_zero_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_tokens = non_zero_tokens.sum(-1)\n",
    "print(non_zero_tokens)\n",
    "print(non_zero_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque document textuel, on somme la valeur de l'ensemble des tokens par dimension d'embedding. A ce stade, chaque document du batch est représenté par un vecteur de taille dim (180 par exemple). Ensuite, pour standardiser ces sommes, on divise chaque vecteur par le nombre de \"vrais tokens\" par documents. A noter qu'on remplace les éventuelles valeurs infinis par des 0 a posteriori : gestion des cas où un document textuel ne contiendrait aucun token de la matrice d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_1.sum(dim=-2))\n",
    "print(x_1.sum(dim=-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_1.sum(dim=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 /= non_zero_tokens.unsqueeze(-1)\n",
    "print(x_1)\n",
    "print(x_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.nan_to_num(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque input du jeu de données, on fait la somme du vecteur d'embedding du document textuel et des vecteurs d'embedding de chacune des variables catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.stack(x_cat, dim=0).sum(dim=0))\n",
    "print(torch.stack(x_cat, dim=0).sum(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if x_cat != []:\n",
    "    x_in = x_1 + torch.stack(x_cat, dim=0).sum(dim=0)\n",
    "else:\n",
    "    x_in = x_1\n",
    "\n",
    "print(x_in)\n",
    "print(x_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on crée une couche linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = self_fc(x_in)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def forward(self, inputs: List[torch.LongTensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "\n",
    "        Args:\n",
    "            inputs (List[torch.LongTensor]): Model inputs.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Model output.\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        x_1 = inputs[0]\n",
    "        x_1 = self.embeddings(x_1)\n",
    "\n",
    "        x_cat = []\n",
    "        for i, (variable, embedding_layer) in enumerate(\n",
    "            self.categorical_embeddings.items()\n",
    "        ):\n",
    "            x_cat.append(embedding_layer(inputs[i + 1]))\n",
    "\n",
    "        # Mean of tokens\n",
    "        non_zero_tokens = x_1.sum(-1) != 0\n",
    "        non_zero_tokens = non_zero_tokens.sum(-1)\n",
    "        x_1 = x_1.sum(dim=-2)\n",
    "        x_1 /= non_zero_tokens.unsqueeze(-1)\n",
    "        x_1 = torch.nan_to_num(x_1)\n",
    "\n",
    "        if x_cat != []:\n",
    "            x_in = x_1 + torch.stack(x_cat, dim=0).sum(dim=0)\n",
    "        else:\n",
    "            x_in = x_1\n",
    "\n",
    "        # Linear layer\n",
    "        z = self.fc(x_in)\n",
    "        return z\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
